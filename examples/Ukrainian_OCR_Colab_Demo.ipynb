{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukrainian-ocr-title"
   },
   "source": [
    "# üá∫üá¶ Ukrainian OCR Pipeline - Google Colab Demo\n",
    "\n",
    "High-performance OCR pipeline for historical Ukrainian documents with Named Entity Recognition (NER).\n",
    "\n",
    "**Features:**\n",
    "- ‚ö° GPU-accelerated TrOCR for Cyrillic handwriting\n",
    "- üéØ Named Entity Recognition for persons and locations\n",
    "- üìã ALTO XML output for archival standards\n",
    "- üé® Person-dense region extraction\n",
    "- üìä Progress tracking and performance monitoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## üöÄ Setup and Installation\n",
    "\n",
    "First, let's install the Ukrainian OCR pipeline package and check GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-package"
   },
   "outputs": [],
   "source": [
    "# Complete setup with automatic dependency and model installation\n",
    "!git clone https://github.com/mary-lev/ukrainian-ocr-pipeline.git\n",
    "%cd ukrainian-ocr-pipeline\n",
    "!pip install -e .[colab] --quiet\n",
    "\n",
    "# Run complete setup with all models (including spaCy)\n",
    "from ukrainian_ocr import setup_complete_colab_environment, check_pytorch_compatibility\n",
    "\n",
    "print(\"üöÄ Setting up Ukrainian OCR Pipeline with all dependencies and models...\")\n",
    "\n",
    "# First check PyTorch compatibility to avoid the disable() error\n",
    "print(\"üî• Checking PyTorch compatibility...\")\n",
    "pytorch_ok = check_pytorch_compatibility()\n",
    "\n",
    "env_info = setup_complete_colab_environment()\n",
    "\n",
    "print(\"‚úÖ Complete setup finished!\")\n",
    "print(f\"üìä Environment: {'GPU' if env_info['has_gpu'] else 'CPU'}\")\n",
    "print(f\"üîß Dependencies: {'‚úÖ' if env_info['dependencies_installed'] else '‚ùå'}\")\n",
    "print(f\"üî• PyTorch Compatible: {'‚úÖ' if env_info.get('pytorch_compatible', False) else '‚ùå'}\")\n",
    "\n",
    "# Alternative: Manual spaCy upgrade (optional, for better NER)\n",
    "# Uncomment the line below if you want to ensure spaCy is installed:\n",
    "# from ukrainian_ocr import upgrade_ner_to_spacy\n",
    "# upgrade_ner_to_spacy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability and setup\n",
    "import ukrainian_ocr\n",
    "from ukrainian_ocr.utils.gpu import check_gpu_availability, setup_colab_gpu\n",
    "\n",
    "# Check GPU\n",
    "gpu_info = setup_colab_gpu()\n",
    "\n",
    "if gpu_info['cuda_available']:\n",
    "    print(f\"üéâ GPU detected: {gpu_info['gpu_names'][0]}\")\n",
    "    print(f\"üíæ GPU Memory: {gpu_info['gpu_memory'][0]:.1f}GB\")\n",
    "    print(f\"üî• Recommended device: {gpu_info['recommended_device']}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Enable GPU: Runtime -> Change runtime type -> GPU\")\n",
    "    print(\"üíª Will use CPU (slower processing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ NER Enhancement (Optional)\n",
    "\n",
    "Upgrade from rule-based to spaCy-based Named Entity Recognition for better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload-section"
   },
   "source": [
    "## üìÅ Upload Images\n",
    "\n",
    "Upload your historical Ukrainian document images for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-images"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create upload directory\n",
    "os.makedirs('/content/images', exist_ok=True)\n",
    "\n",
    "# Upload files\n",
    "print(\"üì§ Select your historical document images to upload:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move uploaded files to images directory\n",
    "for filename in uploaded.keys():\n",
    "    os.rename(filename, f'/content/images/{filename}')\n",
    "    print(f\"‚úÖ Uploaded: {filename}\")\n",
    "\n",
    "# List uploaded files\n",
    "image_files = [f'/content/images/{f}' for f in os.listdir('/content/images')]\n",
    "print(f\"\\nüìä Total images uploaded: {len(image_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config-section"
   },
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "Configure the OCR pipeline for optimal performance in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-config"
   },
   "outputs": [],
   "source": [
    "from ukrainian_ocr import UkrainianOCRPipeline, OCRConfig\n",
    "\n",
    "# Create optimized configuration for Colab\n",
    "config = OCRConfig()\n",
    "config.update_for_colab()  # Optimize for Colab environment\n",
    "\n",
    "# Customize settings if needed\n",
    "config.verbose = True  # Enable progress bars\n",
    "config.save_intermediate = True  # Save visualization images\n",
    "config.post_processing.extract_person_regions = True  # Extract person-dense regions\n",
    "\n",
    "# Advanced NER Configuration Examples:\n",
    "# Option 1: Use spaCy (default, best for most cases)\n",
    "config.ner.backend = \"spacy\"\n",
    "config.ner.model_name = \"ru_core_news_lg\"\n",
    "\n",
    "# Option 2: Use Transformers (RoBERTa-large Russian)\n",
    "# config.ner.backend = \"transformers\" \n",
    "# config.ner.model_name = \"roberta_large_russian\"\n",
    "\n",
    "# Option 3: Use OpenAI (requires API key)\n",
    "# config.ner.backend = \"openai\"\n",
    "# config.ner.api_key = \"your-openai-api-key\"\n",
    "\n",
    "# Option 4: Use rule-based (fast, no additional models)\n",
    "# config.ner.backend = \"rule_based\"\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration settings:\")\n",
    "print(f\"  Main device: {config.device}\")\n",
    "print(f\"  OCR device: {config.ocr.device}\")\n",
    "print(f\"  Segmentation device: {config.segmentation.device}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  NER backend: {config.ner.backend}\")\n",
    "print(f\"  NER model: {config.ner.model_name}\")\n",
    "print(f\"  Extract person regions: {config.post_processing.extract_person_regions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "processing-section"
   },
   "source": [
    "## üîÑ Processing Pipeline\n",
    "\n",
    "Initialize the pipeline and process your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init-pipeline"
   },
   "outputs": [],
   "source": [
    "# Initialize the OCR pipeline\n",
    "print(\"üöÄ Initializing Ukrainian OCR Pipeline...\")\n",
    "pipeline = UkrainianOCRPipeline(\n",
    "    config=config,\n",
    "    device='auto',  # Auto-detect best device\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Pipeline initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "process-images"
   },
   "outputs": [],
   "source": [
    "# Process all uploaded images\n",
    "import time\n",
    "\n",
    "print(f\"üîÑ Processing {len(image_files)} image(s)...\\n\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '/content/ocr_results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Start processing\n",
    "start_time = time.time()\n",
    "\n",
    "if len(image_files) == 1:\n",
    "    # Single image processing\n",
    "    results = [pipeline.process_single_image(\n",
    "        image_files[0], \n",
    "        output_dir=output_dir,\n",
    "        save_intermediate=True\n",
    "    )]\n",
    "else:\n",
    "    # Batch processing\n",
    "    results = pipeline.process_batch(\n",
    "        image_files, \n",
    "        output_dir=output_dir,\n",
    "        save_intermediate=True\n",
    "    )\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Display results summary\n",
    "successful = sum(1 for r in results if r['success'])\n",
    "failed = len(results) - successful\n",
    "\n",
    "print(f\"\\nüéâ Processing complete!\")\n",
    "print(f\"‚úÖ Successful: {successful}/{len(results)}\")\n",
    "print(f\"‚ùå Failed: {failed}/{len(results)}\")\n",
    "print(f\"‚è±Ô∏è Total time: {total_time:.1f}s\")\n",
    "print(f\"üìä Average per image: {total_time/len(results):.1f}s\")\n",
    "\n",
    "# Show pipeline statistics\n",
    "stats = pipeline.get_stats()\n",
    "print(f\"\\nüìà Pipeline Statistics:\")\n",
    "print(f\"  Images processed: {stats['images_processed']}\")\n",
    "print(f\"  Total processing time: {stats['total_processing_time']:.1f}s\")\n",
    "print(f\"  Average time per image: {stats['average_time_per_image']:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results-section"
   },
   "source": [
    "## üìä Results Analysis\n",
    "\n",
    "Analyze the processing results and view extracted entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze-results"
   },
   "outputs": [],
   "source": [
    "# Analyze results for each processed image\n",
    "from IPython.display import display, Image, HTML\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    if result['success']:\n",
    "        print(f\"\\nüìÑ Image {i+1}: {result['image_path']}\")\n",
    "        print(f\"‚è±Ô∏è Processing time: {result['processing_time']:.2f}s\")\n",
    "        print(f\"üìè Lines detected: {result['lines_detected']}\")\n",
    "        print(f\"üìù Lines with text: {result['lines_with_text']}\")\n",
    "        \n",
    "        # Show output files\n",
    "        print(f\"\\nüìÅ Output files:\")\n",
    "        for file_type, path in result['output_paths'].items():\n",
    "            if path and os.path.exists(path):\n",
    "                size_mb = os.path.getsize(path) / 1024 / 1024\n",
    "                print(f\"  {file_type}: {os.path.basename(path)} ({size_mb:.1f}MB)\")\n",
    "                \n",
    "        # Try to extract entities from enhanced ALTO\n",
    "        alto_enhanced = result['output_paths'].get('alto_enhanced')\n",
    "        if alto_enhanced and os.path.exists(alto_enhanced):\n",
    "            try:\n",
    "                tree = ET.parse(alto_enhanced)\n",
    "                root = tree.getroot()\n",
    "                \n",
    "                # Count entity lines\n",
    "                person_lines = len(root.findall('.//*[@ENTITY_TYPES=\"PERSON\"]'))\n",
    "                location_lines = len(root.findall('.//*[@ENTITY_TYPES=\"LOCATION\"]'))\n",
    "                \n",
    "                print(f\"\\nüéØ Entities extracted:\")\n",
    "                print(f\"  üë§ Person lines: {person_lines}\")\n",
    "                print(f\"  üìç Location lines: {location_lines}\")\n",
    "                \n",
    "                # Check for person-dense regions\n",
    "                dense_blocks = root.findall('.//TextBlock[@PERSON_LINES_COUNT]')\n",
    "                if dense_blocks:\n",
    "                    for block in dense_blocks:\n",
    "                        person_count = block.get('PERSON_LINES_COUNT', 0)\n",
    "                        print(f\"  üéØ Person-dense region: {person_count} person lines\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Could not parse ALTO file: {e}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Image {i+1} failed: {result.get('error', 'Unknown error')}\")\n",
    "        \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced NER Results Analysis\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    if result['success']:\n",
    "        print(f\"\\nüéØ Advanced NER Analysis - Image {i+1}: {result['image_path']}\")\n",
    "        print(f\"‚öôÔ∏è NER Backend: {result.get('ner_backend', 'Unknown')}\")\n",
    "        \n",
    "        # Show entity statistics\n",
    "        entities_extracted = result.get('entities_extracted', 0)\n",
    "        total_entities = result.get('total_entities', 0)\n",
    "        \n",
    "        if total_entities > 0:\n",
    "            print(f\"üìä Entity Statistics:\")\n",
    "            print(f\"  Lines with entities: {entities_extracted}\")\n",
    "            print(f\"  Total entities found: {total_entities}\")\n",
    "            \n",
    "            # Try to load and analyze the enhanced ALTO\n",
    "            alto_enhanced = result['output_paths'].get('alto_enhanced')\n",
    "            if alto_enhanced and os.path.exists(alto_enhanced):\n",
    "                try:\n",
    "                    import xml.etree.ElementTree as ET\n",
    "                    tree = ET.parse(alto_enhanced)\n",
    "                    root = tree.getroot()\n",
    "                    \n",
    "                    # Find lines with different entity types\n",
    "                    person_lines = len(root.findall('.//*[@ENTITY_TYPES]'))\n",
    "                    \n",
    "                    # Count entities by type from the enhanced ALTO\n",
    "                    entity_types = {}\n",
    "                    for elem in root.findall('.//*[@ENTITY_TYPES]'):\n",
    "                        entity_type = elem.get('ENTITY_TYPES', '')\n",
    "                        entity_types[entity_type] = entity_types.get(entity_type, 0) + 1\n",
    "                    \n",
    "                    if entity_types:\n",
    "                        print(f\"\\nüè∑Ô∏è Entities by Type:\")\n",
    "                        for entity_type, count in sorted(entity_types.items()):\n",
    "                            print(f\"  {entity_type}: {count} occurrences\")\n",
    "                    \n",
    "                    # Show sample entities\n",
    "                    print(f\"\\nüìù Sample Entities Found:\")\n",
    "                    sample_count = 0\n",
    "                    for elem in root.findall('.//*[@ENTITY_TYPES]'):\n",
    "                        if sample_count >= 5:  # Show max 5 samples\n",
    "                            break\n",
    "                        text = elem.get('CONTENT', '').strip()\n",
    "                        entity_type = elem.get('ENTITY_TYPES', '')\n",
    "                        if text and entity_type:\n",
    "                            print(f\"  ‚Ä¢ {text} ({entity_type})\")\n",
    "                            sample_count += 1\n",
    "                            \n",
    "                except Exception as parse_error:\n",
    "                    print(f\"  ‚ö†Ô∏è Could not analyze enhanced ALTO: {parse_error}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"  ‚ÑπÔ∏è No entities detected in this document\")\n",
    "        \n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization-section"
   },
   "source": [
    "## üé® Visualization\n",
    "\n",
    "Display processing visualizations and person-dense regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show-visualizations"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image as IPImage, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Display visualizations for successful results\n",
    "for i, result in enumerate(results[:3]):  # Limit to first 3 images\n",
    "    if result['success']:\n",
    "        print(f\"\\nüé® Visualizations for Image {i+1}\")\n",
    "        \n",
    "        # Show segmentation visualization if available\n",
    "        viz_path = result['output_paths'].get('visualization')\n",
    "        if viz_path and os.path.exists(viz_path):\n",
    "            try:\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                img = mpimg.imread(viz_path)\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"Segmentation Results - {os.path.basename(result['image_path'])}\")\n",
    "                plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Could not display visualization: {e}\")\n",
    "                \n",
    "        # Show person region if available\n",
    "        person_regions_path = result['output_paths'].get('person_regions')\n",
    "        if person_regions_path and os.path.exists(person_regions_path):\n",
    "            try:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                img = mpimg.imread(person_regions_path)\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"Person-Dense Region - {os.path.basename(result['image_path'])}\")\n",
    "                plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Could not display person region: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-section"
   },
   "source": [
    "## üíæ Download Results\n",
    "\n",
    "Package and download your processing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-results"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Create a zip file with all results\n",
    "zip_path = '/content/ukrainian_ocr_results.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add all files from results directory\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arc_path = os.path.relpath(file_path, '/content')\n",
    "            zipf.write(file_path, arc_path)\n",
    "\n",
    "zip_size_mb = os.path.getsize(zip_path) / 1024 / 1024\n",
    "print(f\"üì¶ Created results archive: ukrainian_ocr_results.zip ({zip_size_mb:.1f}MB)\")\n",
    "\n",
    "# Download the zip file\n",
    "files.download(zip_path)\n",
    "print(\"‚úÖ Results downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup-section"
   },
   "source": [
    "## üßπ Cleanup\n",
    "\n",
    "Clean up GPU memory and temporary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup"
   },
   "outputs": [],
   "source": [
    "# Clean up pipeline resources\n",
    "pipeline.cleanup()\n",
    "\n",
    "# Show final memory usage\n",
    "from ukrainian_ocr.utils.gpu import monitor_gpu_memory\n",
    "\n",
    "if gpu_info['cuda_available']:\n",
    "    memory_stats = monitor_gpu_memory()\n",
    "    for gpu_id, stats in memory_stats.items():\n",
    "        print(f\"üìä {gpu_id.upper()} Memory Usage:\")\n",
    "        print(f\"  Allocated: {stats['allocated']:.1f}GB\")\n",
    "        print(f\"  Utilization: {stats['utilization']:.1f}%\")\n",
    "\n",
    "print(\"\\nüéâ Ukrainian OCR Pipeline processing complete!\")\n",
    "print(\"üìö Check the downloaded archive for all your results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next-steps"
   },
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "**What you can do with the results:**\n",
    "\n",
    "1. **ALTO XML files** - Import into eScriptorium or other document analysis tools\n",
    "2. **Enhanced ALTO** - Contains named entity annotations for persons and locations\n",
    "3. **Person-dense regions** - Cropped images focusing on genealogically valuable content\n",
    "4. **Entity extraction** - Use the identified persons and locations for genealogical research\n",
    "\n",
    "**For production use:**\n",
    "- Install locally: `pip install ukrainian-ocr-pipeline[all]`\n",
    "- Use CLI interface: `ukrainian-ocr --help`\n",
    "- Customize configuration files\n",
    "- Integrate with existing workflows\n",
    "\n",
    "**Need help?**\n",
    "- üìñ Documentation: [Link to docs]\n",
    "- üêõ Issues: [Link to GitHub issues]\n",
    "- üí¨ Discussions: [Link to discussions]\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
